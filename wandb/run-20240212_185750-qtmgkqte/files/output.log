> /home-nfs/fx2024/NLP/NLP/main.py(110)gpt_finetune()
-> num_batches = len(dataloader)
tensor([[  383,  8765, 11819,  ..., 11819,   837,   318],
        [  796,   796,  7712,  ..., 50256, 50256, 50256],
        [  383,   983,   705,  ...,  3264,   422,   569],
        ...,
        [  796,   796, 20395,  ..., 50256, 50256, 50256],
        [ 1081,   262, 17871,  ...,   666,  5407, 14561],
        [ 1081,   351,  2180,  ...,  2597,  2488,    12]], device='cuda:0') tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')
epoch_idx: 0
batch_idx: 0
> /home-nfs/fx2024/NLP/NLP/main.py(122)gpt_finetune()
-> loss = model(batch_input_ids, attention_mask=batch_attention_mask, labels=batch_input_ids).loss.sum()
/home-nfs/fx2024/mc3/envs/fei-venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
loss tensor(15.5661, device='cuda:0', grad_fn=<SumBackward0>)
checkpoint saved for epoch 0 batch count total 1
> /home-nfs/fx2024/NLP/NLP/main.py(159)gpt_finetune()
-> if not os.path.exists(save_dir):
checkpoint saved for epoch 0
> /home-nfs/fx2024/NLP/NLP/main.py(110)gpt_finetune()
-> num_batches = len(dataloader)
tensor([[14047, 15934, 11175,  ...,  6573,  5055,   837],
        [ 8498,  2840,   389,  ..., 20811,   290, 29347],
        [  383,  1578,  1829,  ...,   262,  3942,  3277],
        ...,
        [50256, 50256, 50256,  ..., 50256, 50256, 50256],
        [50256, 50256, 50256,  ..., 50256, 50256, 50256],
        [  796,   796, 20395,  ..., 50256, 50256, 50256]], device='cuda:0') tensor([[1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')
epoch_idx: 1
batch_idx: 0
> /home-nfs/fx2024/NLP/NLP/main.py(122)gpt_finetune()
-> loss = model(batch_input_ids, attention_mask=batch_attention_mask, labels=batch_input_ids).loss.sum()
loss tensor(9.8539, device='cuda:0', grad_fn=<SumBackward0>)
checkpoint saved for epoch 1 batch count total 1
> /home-nfs/fx2024/NLP/NLP/main.py(159)gpt_finetune()
-> if not os.path.exists(save_dir):
checkpoint saved for epoch 1
> /home-nfs/fx2024/NLP/NLP/main.py(110)gpt_finetune()
-> num_batches = len(dataloader)
tensor([[  383,  2615, 11583,  ...,  4645,   286,   262],
        [50256, 50256, 50256,  ..., 50256, 50256, 50256],
        [  383,  1578,  1829,  ...,   262,  3942,  3277],
        ...,
        [50256, 50256, 50256,  ..., 50256, 50256, 50256],
        [50256, 50256, 50256,  ..., 50256, 50256, 50256],
        [50256, 50256, 50256,  ..., 50256, 50256, 50256]], device='cuda:0') tensor([[1, 1, 1,  ..., 1, 1, 1],
        [0, 0, 0,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 1, 1, 1],
        ...,
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0]], device='cuda:0')
epoch_idx: 2
batch_idx: 0
> /home-nfs/fx2024/NLP/NLP/main.py(122)gpt_finetune()
-> loss = model(batch_input_ids, attention_mask=batch_attention_mask, labels=batch_input_ids).loss.sum()
loss tensor(6.1199, device='cuda:0', grad_fn=<SumBackward0>)
checkpoint saved for epoch 2 batch count total 1
> /home-nfs/fx2024/NLP/NLP/main.py(159)gpt_finetune()
-> if not os.path.exists(save_dir):
checkpoint saved for epoch 2
> /home-nfs/fx2024/NLP/NLP/main.py(110)gpt_finetune()
-> num_batches = len(dataloader)
