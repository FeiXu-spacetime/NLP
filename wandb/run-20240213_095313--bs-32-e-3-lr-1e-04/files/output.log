
  0%|                                                                                                                                                   | 0/59 [00:00<?, ?it/s]
epoch_idx: 0
  0%|                                                                                                                                                   | 0/59 [00:00<?, ?it/s]/home-nfs/fx2024/mc3/envs/fei-venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
  2%|██▎                                                                                                                                        | 1/59 [00:03<03:07,  3.23s/it]
loss tensor(16.3353, device='cuda:0', grad_fn=<SumBackward0>)
epoch_idx: 0

  5%|███████                                                                                                                                    | 3/59 [00:04<01:18,  1.41s/it]
loss tensor(15.2287, device='cuda:0', grad_fn=<SumBackward0>)
epoch_idx: 0
batch_idx: 2
loss tensor(15.0866, device='cuda:0', grad_fn=<SumBackward0>)
epoch_idx: 0
batch_idx: 3

loss tensor(15.2879, device='cuda:0', grad_fn=<SumBackward0>)
epoch_idx: 0
batch_idx: 4
loss tensor(15.1750, device='cuda:0', grad_fn=<SumBackward0>)
epoch_idx: 0
batch_idx: 5

 14%|██████████████████▊                                                                                                                        | 8/59 [00:09<00:48,  1.06it/s]
epoch_idx: 0
batch_idx: 6
loss tensor(15.2110, device='cuda:0', grad_fn=<SumBackward0>)
epoch_idx: 0
batch_idx: 7
loss tensor(14.5529, device='cuda:0', grad_fn=<SumBackward0>)
epoch_idx: 0

 17%|███████████████████████▍                                                                                                                  | 10/59 [00:11<00:44,  1.10it/s]
loss tensor(13.8998, device='cuda:0', grad_fn=<SumBackward0>)
epoch_idx: 0
batch_idx: 9
loss tensor(14.4310, device='cuda:0', grad_fn=<SumBackward0>)
epoch_idx: 0

 20%|████████████████████████████                                                                                                              | 12/59 [00:12<00:42,  1.10it/s]
loss tensor(14.5407, device='cuda:0', grad_fn=<SumBackward0>)
epoch_idx: 0
batch_idx: 11
loss tensor(13.9245, device='cuda:0', grad_fn=<SumBackward0>)
epoch_idx: 0
batch_idx: 12

 24%|████████████████████████████████▋                                                                                                         | 14/59 [00:14<00:39,  1.13it/s]
epoch_idx: 0
batch_idx: 13
loss tensor(14.3065, device='cuda:0', grad_fn=<SumBackward0>)
epoch_idx: 0
batch_idx: 14

 29%|███████████████████████████████████████▊                                                                                                  | 17/59 [00:17<00:37,  1.13it/s]
epoch_idx: 0
batch_idx: 15
loss tensor(14.0447, device='cuda:0', grad_fn=<SumBackward0>)
epoch_idx: 0
batch_idx: 16
loss tensor(14.0119, device='cuda:0', grad_fn=<SumBackward0>)
epoch_idx: 0

 32%|████████████████████████████████████████████▍                                                                                             | 19/59 [00:19<00:35,  1.14it/s]
loss tensor(13.6914, device='cuda:0', grad_fn=<SumBackward0>)
epoch_idx: 0
batch_idx: 18
loss tensor(14.2805, device='cuda:0', grad_fn=<SumBackward0>)
epoch_idx: 0
 36%|█████████████████████████████████████████████████                                                                                         | 21/59 [00:20<00:37,  1.00it/s]
Traceback (most recent call last):
  File "/home-nfs/fx2024/NLP/NLP/main.py", line 348, in <module>
    gpt_finetune(args, model)
  File "/home-nfs/fx2024/NLP/NLP/main.py", line 141, in gpt_finetune
    loss = model(batch_input_ids, attention_mask=batch_attention_mask, labels=batch_input_ids).loss.sum()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home-nfs/fx2024/mc3/envs/fei-venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home-nfs/fx2024/mc3/envs/fei-venv/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home-nfs/fx2024/mc3/envs/fei-venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py", line 184, in forward
    replicas = self.replicate(self.module, self.device_ids[:len(inputs)])
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home-nfs/fx2024/mc3/envs/fei-venv/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py", line 189, in replicate
    return replicate(module, device_ids, not torch.is_grad_enabled())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home-nfs/fx2024/mc3/envs/fei-venv/lib/python3.11/site-packages/torch/nn/parallel/replicate.py", line 110, in replicate
    param_copies = _broadcast_coalesced_reshape(params, devices, detach)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home-nfs/fx2024/mc3/envs/fei-venv/lib/python3.11/site-packages/torch/nn/parallel/replicate.py", line 83, in _broadcast_coalesced_reshape
    tensor_copies = Broadcast.apply(devices, *tensors)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home-nfs/fx2024/mc3/envs/fei-venv/lib/python3.11/site-packages/torch/autograd/function.py", line 553, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home-nfs/fx2024/mc3/envs/fei-venv/lib/python3.11/site-packages/torch/nn/parallel/_functions.py", line 23, in forward
    outputs = comm.broadcast_coalesced(inputs, ctx.target_gpus)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home-nfs/fx2024/mc3/envs/fei-venv/lib/python3.11/site-packages/torch/nn/parallel/comm.py", line 57, in broadcast_coalesced
    return torch._C._broadcast_coalesced(tensors, devices, buffer_size)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
loss tensor(13.7003, device='cuda:0', grad_fn=<SumBackward0>)
epoch_idx: 0
batch_idx: 20
loss tensor(13.7722, device='cuda:0', grad_fn=<SumBackward0>)
epoch_idx: 0
batch_idx: 21